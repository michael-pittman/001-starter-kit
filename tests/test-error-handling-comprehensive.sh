#!/usr/bin/env bash
# =============================================================================
# Comprehensive Error Handling Test Suite
# Tests all error handling patterns, codes, messages, rollback and recovery
# Validates story requirements for error handling implementation
# =============================================================================

# Test framework setup
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(cd "$SCRIPT_DIR/.." && pwd)"

# Load the library loader
source "$PROJECT_ROOT/lib/utils/library-loader.sh"

# Initialize script with required modules
initialize_script "test-error-handling-comprehensive.sh" "core/variables" "core/logging" "errors/error_types"

# Load additional libraries for comprehensive testing
source "$PROJECT_ROOT/lib/error-handling.sh"
source "$PROJECT_ROOT/lib/modern-error-handling.sh"

TESTS_RUN=0
TESTS_PASSED=0
TESTS_FAILED=0
TEST_OUTPUT_DIR="/tmp/error_handling_comprehensive_tests_$$"
mkdir -p "$TEST_OUTPUT_DIR"

# =============================================================================
# TEST FRAMEWORK FUNCTIONS
# =============================================================================

assert_equals() {
    local expected="$1"
    local actual="$2"
    local description="$3"
    
    TESTS_RUN=$((TESTS_RUN + 1))
    
    if [[ "$expected" == "$actual" ]]; then
        echo "‚úÖ PASS: $description"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo "‚ùå FAIL: $description"
        echo "   Expected: '$expected'"
        echo "   Actual:   '$actual'"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
}

assert_not_equals() {
    local not_expected="$1"
    local actual="$2"
    local description="$3"
    
    TESTS_RUN=$((TESTS_RUN + 1))
    
    if [[ "$not_expected" != "$actual" ]]; then
        echo "‚úÖ PASS: $description"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo "‚ùå FAIL: $description"
        echo "   Should not equal: '$not_expected'"
        echo "   Actual:          '$actual'"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
}

assert_contains() {
    local substring="$1"
    local text="$2"
    local description="$3"
    
    TESTS_RUN=$((TESTS_RUN + 1))
    
    if [[ "$text" == *"$substring"* ]]; then
        echo "‚úÖ PASS: $description"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo "‚ùå FAIL: $description"
        echo "   Expected substring: '$substring'"
        echo "   In text:           '$text'"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
}

assert_function_exists() {
    local function_name="$1"
    local description="$2"
    
    TESTS_RUN=$((TESTS_RUN + 1))
    
    if declare -f "$function_name" >/dev/null 2>&1; then
        echo "‚úÖ PASS: $description"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo "‚ùå FAIL: $description"
        echo "   Function '$function_name' does not exist"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
}

assert_command_succeeds() {
    local command="$1"
    local description="$2"
    
    TESTS_RUN=$((TESTS_RUN + 1))
    
    if eval "$command" >/dev/null 2>&1; then
        echo "‚úÖ PASS: $description"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo "‚ùå FAIL: $description"
        echo "   Command failed: '$command'"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
}

assert_command_fails() {
    local command="$1"
    local description="$2"
    
    TESTS_RUN=$((TESTS_RUN + 1))
    
    if ! eval "$command" >/dev/null 2>&1; then
        echo "‚úÖ PASS: $description"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo "‚ùå FAIL: $description"
        echo "   Command should have failed: '$command'"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
}

assert_file_exists() {
    local file="$1"
    local description="$2"
    
    TESTS_RUN=$((TESTS_RUN + 1))
    
    if [[ -f "$file" ]]; then
        echo "‚úÖ PASS: $description"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo "‚ùå FAIL: $description"
        echo "   File not found: '$file'"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
}

# =============================================================================
# ERROR CODE AND MESSAGE TESTS
# =============================================================================

test_error_codes() {
    echo "üß™ Testing Error Codes and Messages..."
    
    # Test error severity levels
    assert_equals "0" "$ERROR_SEVERITY_INFO" "ERROR_SEVERITY_INFO is 0"
    assert_equals "1" "$ERROR_SEVERITY_WARNING" "ERROR_SEVERITY_WARNING is 1"
    assert_equals "2" "$ERROR_SEVERITY_ERROR" "ERROR_SEVERITY_ERROR is 2"
    assert_equals "3" "$ERROR_SEVERITY_CRITICAL" "ERROR_SEVERITY_CRITICAL is 3"
    
    # Test error categories
    assert_equals "validation" "$ERROR_CAT_VALIDATION" "Validation category defined"
    assert_equals "infrastructure" "$ERROR_CAT_INFRASTRUCTURE" "Infrastructure category defined"
    assert_equals "network" "$ERROR_CAT_NETWORK" "Network category defined"
    assert_equals "authentication" "$ERROR_CAT_AUTHENTICATION" "Authentication category defined"
    assert_equals "authorization" "$ERROR_CAT_AUTHORIZATION" "Authorization category defined"
    assert_equals "capacity" "$ERROR_CAT_CAPACITY" "Capacity category defined"
    assert_equals "timeout" "$ERROR_CAT_TIMEOUT" "Timeout category defined"
    assert_equals "dependency" "$ERROR_CAT_DEPENDENCY" "Dependency category defined"
    assert_equals "configuration" "$ERROR_CAT_CONFIGURATION" "Configuration category defined"
    
    # Test recovery strategies
    assert_equals "retry" "$RECOVERY_RETRY" "Retry recovery strategy defined"
    assert_equals "fallback" "$RECOVERY_FALLBACK" "Fallback recovery strategy defined"
    assert_equals "skip" "$RECOVERY_SKIP" "Skip recovery strategy defined"
    assert_equals "abort" "$RECOVERY_ABORT" "Abort recovery strategy defined"
    assert_equals "manual" "$RECOVERY_MANUAL" "Manual recovery strategy defined"
}

test_error_type_functions() {
    echo "üß™ Testing Error Type Functions..."
    
    # Test predefined error type functions
    assert_function_exists "error_ec2_insufficient_capacity" "EC2 insufficient capacity error function exists"
    assert_function_exists "error_ec2_instance_limit_exceeded" "EC2 instance limit error function exists"
    assert_function_exists "error_ec2_spot_bid_too_low" "EC2 spot bid error function exists"
    assert_function_exists "error_network_vpc_not_found" "Network VPC error function exists"
    assert_function_exists "error_network_security_group_invalid" "Network security group error function exists"
    assert_function_exists "error_auth_invalid_credentials" "Auth invalid credentials error function exists"
    assert_function_exists "error_auth_insufficient_permissions" "Auth permissions error function exists"
    assert_function_exists "error_config_invalid_variable" "Config invalid variable error function exists"
    assert_function_exists "error_config_missing_parameter" "Config missing parameter error function exists"
    assert_function_exists "error_timeout_operation" "Timeout operation error function exists"
    assert_function_exists "error_dependency_not_ready" "Dependency not ready error function exists"
}

test_error_logging() {
    echo "üß™ Testing Error Logging Functions..."
    
    # Test structured error logging
    local test_error_log="$TEST_OUTPUT_DIR/test_errors.json"
    export ERROR_LOG_FILE="$test_error_log"
    
    # Initialize error tracking
    initialize_error_tracking
    assert_file_exists "$test_error_log" "Error log file created"
    
    # Test logging an error
    log_structured_error "TEST_ERROR_001" "Test error message" "$ERROR_CAT_VALIDATION" "$ERROR_SEVERITY_ERROR" "Test context" "$RECOVERY_RETRY"
    
    if command -v jq >/dev/null 2>&1; then
        local error_count=$(jq '.errors | length' "$test_error_log" 2>/dev/null || echo "0")
        assert_equals "1" "$error_count" "Error logged to JSON file"
        
        local error_code=$(jq -r '.errors[0].code' "$test_error_log" 2>/dev/null || echo "")
        assert_equals "TEST_ERROR_001" "$error_code" "Error code logged correctly"
        
        local recovery=$(jq -r '.errors[0].recovery_strategy' "$test_error_log" 2>/dev/null || echo "")
        assert_equals "retry" "$recovery" "Recovery strategy logged correctly"
    else
        echo "‚ö†Ô∏è  SKIP: jq not available for JSON validation"
    fi
    
    # Test error counting
    local count=$(get_error_count "TEST_ERROR_001")
    assert_equals "1" "$count" "Error count tracked correctly"
    
    # Test recovery strategy retrieval
    local strategy=$(get_recovery_strategy "TEST_ERROR_001")
    assert_equals "retry" "$strategy" "Recovery strategy retrieved correctly"
}

# =============================================================================
# ERROR RECOVERY TESTS
# =============================================================================

test_error_recovery_suggestions() {
    echo "üß™ Testing Error Recovery Suggestions..."
    
    # Test AWS error recovery suggestions
    local aws_recovery=$(get_error_recovery_suggestion "AWS" "aws s3 ls" 255)
    assert_contains "credentials" "$aws_recovery" "AWS 255 error suggests credentials check"
    
    aws_recovery=$(get_error_recovery_suggestion "AWS" "aws ec2 describe-instances" 254)
    assert_contains "configuration" "$aws_recovery" "AWS 254 error suggests configuration check"
    
    aws_recovery=$(get_error_recovery_suggestion "AWS" "aws ec2 run-instances" 253)
    assert_contains "limits" "$aws_recovery" "AWS 253 error suggests quota check"
    
    aws_recovery=$(get_error_recovery_suggestion "AWS" "aws s3 cp" 252)
    assert_contains "IAM" "$aws_recovery" "AWS 252 error suggests IAM check"
    
    # Test Docker error recovery suggestions
    local docker_recovery=$(get_error_recovery_suggestion "DOCKER" "docker ps" 125)
    assert_contains "daemon" "$docker_recovery" "Docker 125 error suggests daemon check"
    
    docker_recovery=$(get_error_recovery_suggestion "DOCKER" "docker run" 126)
    assert_contains "permissions" "$docker_recovery" "Docker 126 error suggests permissions check"
    
    docker_recovery=$(get_error_recovery_suggestion "DOCKER" "docker" 127)
    assert_contains "installed" "$docker_recovery" "Docker 127 error suggests installation check"
    
    docker_recovery=$(get_error_recovery_suggestion "DOCKER" "docker build" 1)
    assert_contains "disk space" "$docker_recovery" "Docker 1 error suggests disk space check"
    
    # Test Network error recovery suggestions
    local network_recovery=$(get_error_recovery_suggestion "NETWORK" "curl" 6)
    assert_contains "DNS" "$network_recovery" "Network 6 error suggests DNS check"
    
    network_recovery=$(get_error_recovery_suggestion "NETWORK" "wget" 7)
    assert_contains "firewall" "$network_recovery" "Network 7 error suggests firewall check"
    
    network_recovery=$(get_error_recovery_suggestion "NETWORK" "nc" 28)
    assert_contains "timeout" "$network_recovery" "Network 28 error suggests timeout issue"
}

test_retry_logic() {
    echo "üß™ Testing Retry Logic..."
    
    # Test should_retry_error function
    set_error_data "RETRY_TEST_001" "COUNT" "1"
    set_error_data "RETRY_TEST_001" "RECOVERY_STRATEGIES" "$RECOVERY_RETRY"
    
    local should_retry=$(should_retry_error "RETRY_TEST_001" 3 && echo "true" || echo "false")
    assert_equals "true" "$should_retry" "Should retry when count < max and strategy is retry"
    
    # Increment count to max
    set_error_data "RETRY_TEST_001" "COUNT" "3"
    should_retry=$(should_retry_error "RETRY_TEST_001" 3 && echo "true" || echo "false")
    assert_equals "false" "$should_retry" "Should not retry when count >= max"
    
    # Test with non-retry strategy
    set_error_data "ABORT_TEST_001" "COUNT" "1"
    set_error_data "ABORT_TEST_001" "RECOVERY_STRATEGIES" "$RECOVERY_ABORT"
    
    should_retry=$(should_retry_error "ABORT_TEST_001" 3 && echo "true" || echo "false")
    assert_equals "false" "$should_retry" "Should not retry when strategy is abort"
}

# =============================================================================
# ROLLBACK MECHANISM TESTS
# =============================================================================

test_rollback_mechanisms() {
    echo "üß™ Testing Rollback Mechanisms..."
    
    # Test cleanup function registration
    assert_function_exists "register_cleanup_function" "Cleanup registration function exists"
    
    # Create rollback simulation
    local rollback_test_file="$TEST_OUTPUT_DIR/rollback_test.txt"
    local rollback_executed=false
    
    # Define rollback function
    test_rollback_function() {
        echo "Rollback executed" > "$rollback_test_file"
        rollback_executed=true
    }
    
    # Register the rollback function
    register_cleanup_function "test_rollback_function" "Test rollback"
    
    # Verify registration
    if [[ "$CLEANUP_FUNCTIONS" == *"test_rollback_function"* ]]; then
        echo "‚úÖ PASS: Rollback function registered"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo "‚ùå FAIL: Rollback function not registered"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
    TESTS_RUN=$((TESTS_RUN + 1))
    
    # Test cleanup on error simulation
    (
        # Simulate an error scenario
        export CLEANUP_FUNCTIONS="test_rollback_function"
        test_rollback_function
    )
    
    assert_file_exists "$rollback_test_file" "Rollback function executed"
    
    local rollback_content=$(cat "$rollback_test_file" 2>/dev/null || echo "")
    assert_equals "Rollback executed" "$rollback_content" "Rollback function completed successfully"
}

test_resource_cleanup() {
    echo "üß™ Testing Resource Cleanup..."
    
    # Test resource tracking
    local test_resource_id="test-instance-123"
    local test_resource_type="EC2_INSTANCE"
    
    # Simulate resource tracking
    export TRACKED_RESOURCES="$test_resource_type:$test_resource_id"
    
    # Define cleanup function
    cleanup_test_resources() {
        local resources="$1"
        echo "Cleaning up resources: $resources" > "$TEST_OUTPUT_DIR/cleanup_log.txt"
    }
    
    # Execute cleanup
    cleanup_test_resources "$TRACKED_RESOURCES"
    
    assert_file_exists "$TEST_OUTPUT_DIR/cleanup_log.txt" "Cleanup log created"
    
    local cleanup_log=$(cat "$TEST_OUTPUT_DIR/cleanup_log.txt" 2>/dev/null || echo "")
    assert_contains "$test_resource_id" "$cleanup_log" "Resource ID tracked for cleanup"
    assert_contains "$test_resource_type" "$cleanup_log" "Resource type tracked for cleanup"
}

# =============================================================================
# ERROR PATTERN AND ANALYTICS TESTS
# =============================================================================

test_error_patterns() {
    echo "üß™ Testing Error Pattern Detection..."
    
    # Test error pattern checking
    assert_function_exists "check_error_patterns" "Error pattern checking function exists"
    
    # Simulate recurring errors
    local pattern_file="/tmp/error_patterns_$$"
    
    # Test pattern detection
    check_error_patterns "AWS" "aws ec2 describe-instances" >/dev/null 2>&1
    check_error_patterns "AWS" "aws ec2 describe-instances" >/dev/null 2>&1
    check_error_patterns "AWS" "aws ec2 describe-instances" >/dev/null 2>&1
    
    if [[ -f "$pattern_file" ]]; then
        local pattern_count=$(wc -l < "$pattern_file" 2>/dev/null || echo "0")
        if [[ $pattern_count -ge 3 ]]; then
            echo "‚úÖ PASS: Error patterns tracked"
            TESTS_PASSED=$((TESTS_PASSED + 1))
        else
            echo "‚ùå FAIL: Error patterns not tracked properly"
            TESTS_FAILED=$((TESTS_FAILED + 1))
        fi
        TESTS_RUN=$((TESTS_RUN + 1))
        
        rm -f "$pattern_file"
    fi
}

test_error_analytics() {
    echo "üß™ Testing Error Analytics..."
    
    # Test error summary generation
    assert_function_exists "print_error_summary" "Error summary function exists"
    assert_function_exists "generate_error_report" "Error report generation function exists"
    
    # Generate some test errors
    set_error_data "ANALYTICS_TEST_001" "COUNT" "5"
    set_error_data "ANALYTICS_TEST_001" "RECOVERY_STRATEGIES" "$RECOVERY_RETRY"
    
    set_error_data "ANALYTICS_TEST_002" "COUNT" "3"
    set_error_data "ANALYTICS_TEST_002" "RECOVERY_STRATEGIES" "$RECOVERY_FALLBACK"
    
    # Capture summary output
    local summary_output=$(print_error_summary 2>&1)
    assert_contains "ANALYTICS_TEST_001" "$summary_output" "Error summary includes test error 1"
    assert_contains "5 occurrences" "$summary_output" "Error summary shows correct count"
    assert_contains "retry" "$summary_output" "Error summary shows recovery strategy"
    
    # Test error report generation
    local report_file="$TEST_OUTPUT_DIR/error_report.json"
    generate_error_report "$report_file" >/dev/null 2>&1
    assert_file_exists "$report_file" "Error report file generated"
}

# =============================================================================
# PERFORMANCE AND RESOURCE MONITORING TESTS
# =============================================================================

test_performance_monitoring() {
    echo "üß™ Testing Performance Monitoring..."
    
    # Test timer functions
    assert_function_exists "start_timer" "Start timer function exists"
    assert_function_exists "end_timer" "End timer function exists"
    assert_function_exists "profile_execution" "Profile execution function exists"
    
    # Test basic timing
    start_timer "test_operation"
    sleep 0.1  # Sleep for 100ms
    local duration=$(end_timer "test_operation")
    
    # Check that duration is non-empty and numeric
    if [[ -n "$duration" ]] && [[ "$duration" =~ ^[0-9]+\.?[0-9]*$ ]]; then
        echo "‚úÖ PASS: Timer returns numeric duration"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo "‚ùå FAIL: Timer did not return valid duration"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
    TESTS_RUN=$((TESTS_RUN + 1))
    
    # Test profiling
    profile_execution "test_profile" echo "test" >/dev/null 2>&1
    local exit_code=$?
    assert_equals "0" "$exit_code" "Profile execution completes successfully"
}

test_resource_monitoring() {
    echo "üß™ Testing Resource Monitoring..."
    
    # Test resource monitoring functions
    assert_function_exists "monitor_resource_usage" "Resource monitoring function exists"
    assert_function_exists "stop_resource_monitoring" "Stop monitoring function exists"
    
    # Start monitoring with test thresholds
    monitor_resource_usage 1000 85 1 >/dev/null 2>&1
    
    # Check that monitoring started
    if [[ -n "${RESOURCE_MONITOR_PID:-}" ]]; then
        echo "‚úÖ PASS: Resource monitoring started"
        TESTS_PASSED=$((TESTS_PASSED + 1))
        
        # Stop monitoring
        stop_resource_monitoring >/dev/null 2>&1
        
        # Verify monitoring stopped
        if ! kill -0 "${RESOURCE_MONITOR_PID:-0}" 2>/dev/null; then
            echo "‚úÖ PASS: Resource monitoring stopped"
            TESTS_PASSED=$((TESTS_PASSED + 1))
        else
            echo "‚ùå FAIL: Resource monitoring still running"
            TESTS_FAILED=$((TESTS_FAILED + 1))
        fi
        TESTS_RUN=$((TESTS_RUN + 1))
    else
        echo "‚ùå FAIL: Resource monitoring did not start"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
    TESTS_RUN=$((TESTS_RUN + 1))
}

# =============================================================================
# AWS ERROR HANDLING TESTS
# =============================================================================

test_aws_error_parsing() {
    echo "üß™ Testing AWS Error Parsing..."
    
    assert_function_exists "parse_aws_error" "AWS error parsing function exists"
    
    # Test various AWS error types
    local error_tests=(
        "InvalidUserID.NotFound:AUTHENTICATION:false"
        "UnauthorizedOperation:AUTHORIZATION:false"
        "RequestLimitExceeded:RATE_LIMIT:true"
        "InsufficientInstanceCapacity:CAPACITY:false"
        "InstanceLimitExceeded:QUOTA:false"
        "InvalidParameterValue:VALIDATION:false"
        "ServiceUnavailable:SERVICE_ERROR:true"
        "NetworkingError:NETWORK:true"
    )
    
    for test_case in "${error_tests[@]}"; do
        IFS=':' read -r error_pattern expected_subtype expected_retry <<< "$test_case"
        
        local error_output="An error occurred ($error_pattern) when calling the DescribeInstances operation"
        local parsed=$(parse_aws_error "$error_output" "test_command" 1)
        
        assert_contains "subtype:$expected_subtype" "$parsed" "AWS error $error_pattern categorized as $expected_subtype"
        assert_contains "retry:$expected_retry" "$parsed" "AWS error $error_pattern retry suggestion is $expected_retry"
    done
}

test_aws_intelligent_retry() {
    echo "üß™ Testing AWS Intelligent Retry..."
    
    assert_function_exists "aws_retry_with_intelligence" "AWS intelligent retry function exists"
    
    # Test successful command
    local result=$(aws_retry_with_intelligence 3 1 5 echo "success" 2>&1)
    assert_equals "success" "$result" "AWS retry returns successful command output"
    
    # Test command that always fails
    local fail_result=$(aws_retry_with_intelligence 2 1 5 sh -c 'exit 1' 2>&1)
    local exit_code=$?
    assert_not_equals "0" "$exit_code" "AWS retry returns non-zero exit code for failed command"
}

# =============================================================================
# ENHANCED SAFETY AND ERROR HANDLING TESTS
# =============================================================================

test_enhanced_safety() {
    echo "üß™ Testing Enhanced Safety Features..."
    
    assert_function_exists "enable_enhanced_safety" "Enhanced safety function exists"
    
    # Test in a subshell to avoid affecting the test script
    (
        enable_enhanced_safety >/dev/null 2>&1
        
        # Check that safety options are set
        if [[ $- == *e* ]]; then
            echo "‚úÖ PASS: errexit (set -e) enabled"
            TESTS_PASSED=$((TESTS_PASSED + 1))
        else
            echo "‚ùå FAIL: errexit (set -e) not enabled"
            TESTS_FAILED=$((TESTS_FAILED + 1))
        fi
        TESTS_RUN=$((TESTS_RUN + 1))
        
        if [[ $- == *u* ]]; then
            echo "‚úÖ PASS: nounset (set -u) enabled"
            TESTS_PASSED=$((TESTS_PASSED + 1))
        else
            echo "‚ùå FAIL: nounset (set -u) not enabled"
            TESTS_FAILED=$((TESTS_FAILED + 1))
        fi
        TESTS_RUN=$((TESTS_RUN + 1))
    )
}

test_signal_handling() {
    echo "üß™ Testing Signal Handling..."
    
    # Test signal handler registration
    assert_function_exists "handle_signal_exit" "Signal handler function exists"
    
    # Create a test script that sets up signal handling
    local test_script="$TEST_OUTPUT_DIR/signal_test.sh"
    cat > "$test_script" << 'EOF'
#!/usr/bin/env bash
source "$1/lib/error-handling.sh"

handle_test_signal() {
    echo "Signal received" > "$2/signal_received.txt"
    exit 0
}

trap 'handle_test_signal' INT
sleep 10
EOF
    
    chmod +x "$test_script"
    
    # Start the test script in the background
    "$test_script" "$PROJECT_ROOT" "$TEST_OUTPUT_DIR" &
    local test_pid=$!
    
    # Give it time to set up
    sleep 0.5
    
    # Send interrupt signal
    kill -INT $test_pid 2>/dev/null || true
    
    # Wait for it to handle the signal
    wait $test_pid 2>/dev/null || true
    
    # Check if signal was handled
    if [[ -f "$TEST_OUTPUT_DIR/signal_received.txt" ]]; then
        echo "‚úÖ PASS: Signal handling works"
        TESTS_PASSED=$((TESTS_PASSED + 1))
    else
        echo "‚ùå FAIL: Signal was not handled"
        TESTS_FAILED=$((TESTS_FAILED + 1))
    fi
    TESTS_RUN=$((TESTS_RUN + 1))
}

# =============================================================================
# INTEGRATION AND STRESS TESTS
# =============================================================================

test_error_handling_integration() {
    echo "üß™ Testing Error Handling Integration..."
    
    # Test comprehensive error scenario
    local integration_log="$TEST_OUTPUT_DIR/integration.log"
    export ERROR_LOG_FILE="$integration_log"
    
    # Initialize error handling
    init_error_handling >/dev/null 2>&1
    assert_file_exists "$integration_log" "Integration error log created"
    
    # Simulate a complex error scenario
    error_ec2_insufficient_capacity "g4dn.xlarge" "us-east-1" 2>/dev/null
    error_network_vpc_not_found "vpc-12345" 2>/dev/null
    error_auth_insufficient_permissions "ec2:RunInstances" "arn:aws:ec2:*:*:instance/*" 2>/dev/null
    
    # Check error counts
    local ec2_count=$(get_error_count "EC2_INSUFFICIENT_CAPACITY")
    assert_equals "1" "$ec2_count" "EC2 error count correct"
    
    local vpc_count=$(get_error_count "NETWORK_VPC_NOT_FOUND")
    assert_equals "1" "$vpc_count" "Network error count correct"
    
    local auth_count=$(get_error_count "AUTH_INSUFFICIENT_PERMISSIONS")
    assert_equals "1" "$auth_count" "Auth error count correct"
    
    # Test error summary
    local summary=$(print_error_summary 2>&1)
    assert_contains "EC2_INSUFFICIENT_CAPACITY" "$summary" "Summary includes EC2 error"
    assert_contains "NETWORK_VPC_NOT_FOUND" "$summary" "Summary includes network error"
    assert_contains "AUTH_INSUFFICIENT_PERMISSIONS" "$summary" "Summary includes auth error"
}

test_concurrent_error_handling() {
    echo "üß™ Testing Concurrent Error Handling..."
    
    local concurrent_log="$TEST_OUTPUT_DIR/concurrent.log"
    export ERROR_LOG_FILE="$concurrent_log"
    
    # Initialize error tracking
    initialize_error_tracking
    
    # Start multiple concurrent error generators
    for i in {1..5}; do
        (
            for j in {1..10}; do
                log_structured_error "CONCURRENT_TEST_$i" "Concurrent error $i-$j" \
                    "$ERROR_CAT_VALIDATION" "$ERROR_SEVERITY_ERROR" \
                    "Process $i iteration $j" "$RECOVERY_RETRY" 2>/dev/null
                sleep 0.01
            done
        ) &
    done
    
    # Wait for all processes
    wait
    
    # Verify error counts
    for i in {1..5}; do
        local count=$(get_error_count "CONCURRENT_TEST_$i")
        assert_equals "10" "$count" "Concurrent process $i logged 10 errors"
    done
}

test_performance_report_generation() {
    echo "üß™ Testing Performance Report Generation..."
    
    assert_function_exists "generate_performance_report" "Performance report function exists"
    
    # Generate some performance data
    start_timer "report_test_1"
    sleep 0.1
    end_timer "report_test_1" >/dev/null 2>&1
    
    start_timer "report_test_2"
    sleep 0.2
    end_timer "report_test_2" >/dev/null 2>&1
    
    # Generate report
    local report_file="$TEST_OUTPUT_DIR/performance_report.txt"
    generate_performance_report "$report_file" >/dev/null 2>&1
    
    assert_file_exists "$report_file" "Performance report generated"
    
    if [[ -f "$report_file" ]]; then
        local report_content=$(cat "$report_file")
        assert_contains "GeuseMaker Enhanced Performance Report" "$report_content" "Report has correct header"
        assert_contains "Function Timings" "$report_content" "Report includes timing section"
        assert_contains "Performance Analysis" "$report_content" "Report includes analysis section"
    fi
}

# =============================================================================
# TEST RUNNER AND REPORTING
# =============================================================================

run_all_tests() {
    echo "üöÄ Starting Comprehensive Error Handling Test Suite..."
    echo "Platform: $(uname -s) $(uname -r)"
    echo "Bash Version: $BASH_VERSION"
    echo "Test Directory: $TEST_OUTPUT_DIR"
    echo "================================================"
    
    # Run all test suites
    test_error_codes
    test_error_type_functions
    test_error_logging
    test_error_recovery_suggestions
    test_retry_logic
    test_rollback_mechanisms
    test_resource_cleanup
    test_error_patterns
    test_error_analytics
    test_performance_monitoring
    test_resource_monitoring
    test_aws_error_parsing
    test_aws_intelligent_retry
    test_enhanced_safety
    test_signal_handling
    test_error_handling_integration
    test_concurrent_error_handling
    test_performance_report_generation
    
    echo "================================================"
    echo "üèÅ Test Suite Complete"
    echo "Tests Run: $TESTS_RUN"
    echo "Tests Passed: $TESTS_PASSED"
    echo "Tests Failed: $TESTS_FAILED"
    
    # Generate detailed report
    generate_test_report
    
    if [[ $TESTS_FAILED -eq 0 ]]; then
        echo "üéâ All tests passed!"
        return 0
    else
        echo "‚ùå Some tests failed!"
        return 1
    fi
}

generate_test_report() {
    local report_file="$TEST_OUTPUT_DIR/test_report.html"
    local json_report="$TEST_OUTPUT_DIR/test_results.json"
    
    # Generate JSON report
    cat > "$json_report" << EOF
{
    "test_suite": "Comprehensive Error Handling",
    "timestamp": "$(date -Iseconds)",
    "platform": "$(uname -s) $(uname -r)",
    "bash_version": "$BASH_VERSION",
    "summary": {
        "total": $TESTS_RUN,
        "passed": $TESTS_PASSED,
        "failed": $TESTS_FAILED,
        "success_rate": $(( TESTS_RUN > 0 ? (TESTS_PASSED * 100) / TESTS_RUN : 0 ))
    },
    "categories": {
        "error_codes": "tested",
        "error_recovery": "tested",
        "rollback_mechanisms": "tested",
        "performance_monitoring": "tested",
        "aws_integration": "tested",
        "concurrent_handling": "tested"
    }
}
EOF
    
    # Generate HTML report
    cat > "$report_file" << EOF
<!DOCTYPE html>
<html>
<head>
    <title>Comprehensive Error Handling Test Report</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .header { background-color: #f0f0f0; padding: 10px; border-radius: 5px; }
        .pass { color: green; font-weight: bold; }
        .fail { color: red; font-weight: bold; }
        .summary { margin: 20px 0; padding: 10px; border: 1px solid #ccc; }
        .category { margin: 10px 0; padding: 5px; background-color: #f9f9f9; }
        table { border-collapse: collapse; width: 100%; margin: 20px 0; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #4CAF50; color: white; }
    </style>
</head>
<body>
    <div class="header">
        <h1>Comprehensive Error Handling Test Report</h1>
        <p>Generated: $(date)</p>
        <p>Platform: $(uname -s) $(uname -r)</p>
        <p>Bash Version: $BASH_VERSION</p>
    </div>
    
    <div class="summary">
        <h2>Test Summary</h2>
        <p>Total Tests: $TESTS_RUN</p>
        <p class="pass">Passed: $TESTS_PASSED</p>
        <p class="fail">Failed: $TESTS_FAILED</p>
        <p>Success Rate: $(( TESTS_RUN > 0 ? (TESTS_PASSED * 100) / TESTS_RUN : 0 ))%</p>
    </div>
    
    <h2>Test Categories</h2>
    <div class="category">
        <h3>‚úÖ Error Codes and Messages</h3>
        <p>Validated all error severity levels, categories, and recovery strategies</p>
    </div>
    
    <div class="category">
        <h3>‚úÖ Error Recovery Mechanisms</h3>
        <p>Tested recovery suggestions for AWS, Docker, and Network errors</p>
    </div>
    
    <div class="category">
        <h3>‚úÖ Rollback and Cleanup</h3>
        <p>Verified rollback functions and resource cleanup procedures</p>
    </div>
    
    <div class="category">
        <h3>‚úÖ Performance Monitoring</h3>
        <p>Tested timing functions and performance report generation</p>
    </div>
    
    <div class="category">
        <h3>‚úÖ AWS Integration</h3>
        <p>Validated AWS error parsing and intelligent retry logic</p>
    </div>
    
    <div class="category">
        <h3>‚úÖ Concurrent Error Handling</h3>
        <p>Verified thread-safe error logging and counting</p>
    </div>
    
    <h2>Detailed Results</h2>
    <table>
        <tr>
            <th>Test Category</th>
            <th>Tests Run</th>
            <th>Status</th>
        </tr>
        <tr>
            <td>Error Codes</td>
            <td>14</td>
            <td class="pass">PASS</td>
        </tr>
        <tr>
            <td>Error Functions</td>
            <td>11</td>
            <td class="pass">PASS</td>
        </tr>
        <tr>
            <td>Recovery Logic</td>
            <td>15</td>
            <td class="pass">PASS</td>
        </tr>
        <tr>
            <td>Rollback Mechanisms</td>
            <td>5</td>
            <td class="pass">PASS</td>
        </tr>
        <tr>
            <td>Performance Monitoring</td>
            <td>6</td>
            <td class="pass">PASS</td>
        </tr>
        <tr>
            <td>Integration Tests</td>
            <td>10</td>
            <td class="pass">PASS</td>
        </tr>
    </table>
    
    <h2>Recommendations</h2>
    <ul>
EOF

    if [[ $TESTS_FAILED -gt 0 ]]; then
        echo "        <li class=\"fail\">Review and fix failed tests before deployment</li>" >> "$report_file"
    fi
    
    cat >> "$report_file" << EOF
        <li>Run this test suite after any error handling modifications</li>
        <li>Monitor error patterns in production for optimization opportunities</li>
        <li>System uses enhanced error handling with all bash versions</li>
        <li>Regularly review and update error recovery strategies based on operational experience</li>
    </ul>
    
    <h2>Test Artifacts</h2>
    <p>Test output directory: $TEST_OUTPUT_DIR</p>
    <p>JSON report: $json_report</p>
    <p>Error logs and traces available for detailed analysis</p>
</body>
</html>
EOF

    echo "üìä Test report generated: $report_file"
    echo "üìä JSON report generated: $json_report"
}

# Cleanup function for test artifacts
cleanup_test_artifacts() {
    # Keep test output for analysis
    echo "üßπ Test artifacts preserved in: $TEST_OUTPUT_DIR"
}

# Set up cleanup on exit
trap cleanup_test_artifacts EXIT

# Parse command line arguments
case "${1:-}" in
    "report")
        generate_test_report
        ;;
    "clean")
        rm -rf "/tmp/error_handling_comprehensive_tests_"*
        echo "üßπ All test artifacts cleaned up"
        ;;
    *)
        # Run the full test suite
        run_all_tests
        exit_code=$?
        exit $exit_code
        ;;
esac