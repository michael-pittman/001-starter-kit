# Modern Docker Compose format (no version field required)
# Uses the Compose Specification (latest)
# Optimized for Docker Compose v2.38.2+

# =============================================================================
# MODELS CONFIGURATION (Docker Compose v2.38.2 Feature - EXPERIMENTAL)
# =============================================================================
# Note: Models configuration is experimental in Docker Compose v2.38.2+
# This provides centralized AI model management across services
# Uncomment when using Docker Compose v2.40+ with full models support

# models:
#   deepseek-r1-8b:
#     image: ollama/ollama:latest
#     environment:
#       - OLLAMA_MODEL=deepseek-r1:8b
#       - OLLAMA_TEMPERATURE=0.7
#       - OLLAMA_MAX_TOKENS=4096
#       - OLLAMA_CONTEXT_WINDOW=32768
#       - OLLAMA_STREAM=true
#       - OLLAMA_KEEP_ALIVE=5m
#       - OLLAMA_NUM_PREDICT=-1
#       - OLLAMA_TOP_K=40
#       - OLLAMA_TOP_P=0.9
#       - OLLAMA_REPEAT_PENALTY=1.1
#     labels:
#       - "model.description=DeepSeek R1 reasoning model optimized for complex tasks"
#       - "model.use_cases=reasoning,coding,analysis"
#       - "model.performance_tier=high"
#     
#   qwen2-5-vl-7b:
#     image: ollama/ollama:latest
#     environment:
#       - OLLAMA_MODEL=qwen2.5-vl:7b
#       - OLLAMA_TEMPERATURE=0.7
#       - OLLAMA_MAX_TOKENS=4096
#       - OLLAMA_CONTEXT_WINDOW=32768
#       - OLLAMA_VISION_ENABLED=true
#       - OLLAMA_IMAGE_MAX_SIZE=1024
#       - OLLAMA_IMAGE_QUALITY=auto
#       - OLLAMA_VISION_DETAIL=high
#     labels:
#       - "model.description=Qwen2.5 Vision-Language model for multimodal tasks"
#       - "model.use_cases=vision,multimodal,image_analysis"
#       - "model.performance_tier=high"
#   
#   arctic-embed-568m:
#     image: ollama/ollama:latest
#     environment:
#       - OLLAMA_MODEL=snowflake-arctic-embed2:568m
#       - OLLAMA_EMBEDDING_SIZE=568
#       - OLLAMA_BATCH_SIZE=32
#       - OLLAMA_NORMALIZE_EMBEDDINGS=true
#       - OLLAMA_POOLING_METHOD=mean
#       - OLLAMA_MAX_SEQ_LENGTH=8192
#     labels:
#       - "model.description=High-performance embedding model for semantic search"
#       - "model.use_cases=embeddings,search,retrieval"
#       - "model.performance_tier=medium"

# =============================================================================
# VOLUMES WITH ENHANCED EFS INTEGRATION (v2.38.2 Optimizations)
# =============================================================================
volumes:
  n8n_storage:
    driver: local
    driver_opts:
      type: "nfs"
      o: "addr=${EFS_DNS:-127.0.0.1},nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,fsc,regional,_netdev"
      device: ":/n8n"
    labels:
      - "service=n8n"
      - "backup=daily"
      - "tier=high-performance"
      - "compose.version=2.38.2"
    
  postgres_storage:
    driver: local
    driver_opts:
      type: "nfs"
      o: "addr=${EFS_DNS:-127.0.0.1},nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,fsc,regional,_netdev"
      device: ":/postgres"
    labels:
      - "service=postgres"
      - "backup=hourly"
      - "tier=critical"
      - "compose.version=2.38.2"
    
  ollama_storage:
    driver: local
    driver_opts:
      type: "nfs"
      o: "addr=${EFS_DNS:-127.0.0.1},nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,fsc,regional,_netdev"
      device: ":/ollama"
    labels:
      - "service=ollama"
      - "backup=weekly"
      - "tier=high-performance"
      - "gpu-optimized=true"
      - "compose.version=2.38.2"
    
  qdrant_storage:
    driver: local
    driver_opts:
      type: "nfs"
      o: "addr=${EFS_DNS:-127.0.0.1},nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,fsc,regional,_netdev"
      device: ":/qdrant"
    labels:
      - "service=qdrant"
      - "backup=daily"
      - "tier=high-performance"
      - "compose.version=2.38.2"
    
  shared_storage:
    driver: local
    driver_opts:
      type: "nfs"
      o: "addr=${EFS_DNS:-127.0.0.1},nfsvers=4.1,rsize=1048576,wsize=1048576,hard,timeo=600,retrans=2,fsc,regional,_netdev"
      device: ":/shared"
    labels:
      - "service=shared"
      - "backup=weekly"
      - "tier=standard"
      - "compose.version=2.38.2"

# =============================================================================
# ENHANCED NETWORKS (v2.38.2 Optimizations)
# =============================================================================
networks:
  demo:
    driver: bridge
    driver_opts:
      com.docker.network.bridge.name: "br-ai-stack"
      com.docker.network.bridge.enable_icc: "true"
      com.docker.network.bridge.enable_ip_masquerade: "true"
      com.docker.network.driver.mtu: "1500"
      # v2.38.2 network enhancements
      com.docker.network.bridge.default_bridge: "false"
      com.docker.network.bridge.enable_ipv6: "false"
      com.docker.network.container_interface_prefix: "eth"
    ipam:
      driver: default
      config:
        - subnet: 172.20.0.0/16
          gateway: 172.20.0.1
          ip_range: 172.20.240.0/20
          aux_addresses:
            postgres: 172.20.0.10
            ollama: 172.20.0.11
            qdrant: 172.20.0.12
            n8n: 172.20.0.13
            crawl4ai: 172.20.0.14
    labels:
      - "project=enhanced-ai-starter-kit"
      - "environment=${ENVIRONMENT:-development}"
      - "compose.version=2.38.2"
    # v2.38.2 network metadata
    x-aws-vpc:
      subnets:
        - "subnet-00dd2e4d"
      security_groups:
        - "sg-6da25332"
    x-performance:
      mtu: 1500
      ipv4_forwarding: true
      bridge_nf_call_iptables: true

# =============================================================================
# SHARED CONFIGURATIONS WITH v2.38.2 ENHANCEMENTS
# =============================================================================
x-n8n: &service-n8n
  image: n8nio/n8n:latest
  pull_policy: always
  networks:
    demo:
      ipv4_address: 172.20.0.13
  environment:
    - DB_TYPE=postgresdb
    - DB_POSTGRESDB_HOST=postgres
    - DB_POSTGRESDB_USER=${POSTGRES_USER}
    - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD}
    - DB_POSTGRESDB_DATABASE=${POSTGRES_DB}
    - N8N_DIAGNOSTICS_ENABLED=false
    - N8N_PERSONALIZATION_ENABLED=false
    - N8N_ENCRYPTION_KEY=${N8N_ENCRYPTION_KEY}
    - N8N_USER_MANAGEMENT_JWT_SECRET=${N8N_USER_MANAGEMENT_JWT_SECRET}
    - OLLAMA_HOST=${OLLAMA_HOST:-ollama:11434}
    - N8N_COMMUNITY_PACKAGES_ALLOW_TOOL_USAGE=true
    - N8N_HOST=${N8N_HOST:-0.0.0.0}
    - N8N_PROTOCOL=${N8N_PROTOCOL:-http}
    - WEBHOOK_URL=${WEBHOOK_URL:-http://localhost:5678}
    - N8N_DEFAULT_BINARY_DATA_MODE=filesystem
    - N8N_CORS_ENABLE=true
    - N8N_CORS_ALLOWED_ORIGINS=${N8N_CORS_ALLOWED_ORIGINS:-*}
    - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true
    # v2.38.2 performance optimizations
    - N8N_EXECUTIONS_MODE=queue
    - N8N_EXECUTIONS_CONCURRENCY_TIMEOUT=300
    - N8N_EXECUTIONS_MEMORY_LIMIT=1024
  env_file:
    - path: .env
      required: false
  deploy:
    resources:
      limits:
        memory: 2G
        cpus: '1.0'
        pids: 1024
      reservations:
        memory: 512M
        cpus: '0.5'
    restart_policy:
      condition: any
      delay: 5s
      max_attempts: 3
      window: 120s
    # v2.38.2 placement preferences
    placement:
      preferences:
        - spread: node.labels.zone
  # Enhanced health check with v2.38.2 features
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:5678/healthz"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 40s
    start_interval: 5s

x-ollama: &service-ollama
  image: ollama/ollama:latest
  container_name: ollama
  networks:
    demo:
      ipv4_address: 172.20.0.11
  restart: unless-stopped
  ports:
    - "11434:11434"
  volumes:
    - ollama_storage:/root/.ollama
  environment:
    - OLLAMA_HOST=0.0.0.0
    - OLLAMA_ORIGINS=*
    # v2.38.2 model management integration
    - OLLAMA_MODELS_PATH=/root/.ollama/models
    - OLLAMA_KEEP_ALIVE=5m
    - OLLAMA_MAX_LOADED_MODELS=3
    - OLLAMA_NUM_PARALLEL=4
    - OLLAMA_FLASH_ATTENTION=true
  # Enhanced health check with v2.38.2 features
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
    interval: 30s
    timeout: 10s
    retries: 3
    start_period: 60s
    start_interval: 10s
  deploy:
    resources:
      limits:
        memory: 4G
        cpus: '2.0'
        pids: 2048
      reservations:
        memory: 1G
        cpus: '0.5'
    restart_policy:
      condition: unless-stopped
      delay: 10s
      max_attempts: 3
      window: 120s
    # v2.38.2 placement preferences
    placement:
      preferences:
        - spread: node.labels.zone

x-init-ollama: &init-ollama
  <<: *service-ollama
  networks: ['demo']
  container_name: ollama-pull-models
  volumes:
    - ollama_storage:/root/.ollama
  entrypoint: /bin/sh
  environment:
    - OLLAMA_HOST=ollama:11434
  command:
    - "-c"
    - |
      sleep 10
      ollama pull deepseek-r1:8b || true
      ollama pull qwen2.5-vl:7b || true
      ollama pull snowflake-arctic-embed2:568m || true
      ollama pull mxbai-embed-large:latest || true
      echo "Model initialization complete"

# =============================================================================
# SERVICES
# =============================================================================
services:
  postgres:
    image: postgres:16-alpine
    hostname: postgres
    networks:
      - demo
    restart: unless-stopped
    environment:
      - POSTGRES_USER=${POSTGRES_USER}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD}
      - POSTGRES_DB=${POSTGRES_DB}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
    volumes:
      - postgres_storage:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -h localhost -U ${POSTGRES_USER} -d ${POSTGRES_DB}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'
    command: ["postgres", "-c", "shared_preload_libraries=pg_stat_statements", "-c", "max_connections=100"]

  n8n-import:
    <<: *service-n8n
    hostname: n8n-import
    container_name: n8n-import
    entrypoint: /bin/sh
    command:
      - "-c"
      - "n8n import:credentials --separate --input=/demo-data/credentials && n8n import:workflow --separate --input=/demo-data/workflows"
    volumes:
      - ./n8n/demo-data:/demo-data
    depends_on:
      postgres:
        condition: service_healthy
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 3

  n8n:
    <<: *service-n8n
    hostname: n8n
    container_name: n8n
    restart: unless-stopped
    ports:
      - "5678:5678"
    volumes:
      - n8n_storage:/home/node/.n8n
      - ./n8n/demo-data:/demo-data
      - ./shared:/data/shared
      - ./n8n/certs:/files/certs:ro
    depends_on:
      postgres:
        condition: service_healthy
      n8n-import:
        condition: service_completed_successfully
    deploy:
      restart_policy:
        condition: on-failure
        delay: 5s
        max_attempts: 3
        window: 120s

  qdrant:
    image: qdrant/qdrant:latest
    hostname: qdrant
    container_name: qdrant
    networks:
      - demo
    restart: unless-stopped
    ports:
      - "6333:6333"
    volumes:
      - qdrant_storage:/qdrant/storage
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:6333/healthz"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: '0.5'
        reservations:
          memory: 256M
          cpus: '0.25'

  ollama:
    <<: *service-ollama
    hostname: ollama
    deploy:
      resources:
        limits:
          memory: 4G
          cpus: '2.0'
        reservations:
          memory: 1G
          cpus: '0.5'
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    devices:
      - "/dev/dri:/dev/dri"
      - "/dev/kfd:/dev/kfd"

  ollama-init:
    <<: *init-ollama
    depends_on:
      - ollama
    deploy:
      restart_policy:
        condition: on-failure
        max_attempts: 2

  crawl4ai:
    image: unclecode/crawl4ai:latest
    container_name: crawl4ai
    hostname: crawl4ai
    restart: unless-stopped
    networks:
      - demo
    ports:
      - "11235:11235"
    environment:
      # LLM Configuration for extraction strategies
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      - DEEPSEEK_API_KEY=${DEEPSEEK_API_KEY}
      - GROQ_API_KEY=${GROQ_API_KEY}
      - TOGETHER_API_KEY=${TOGETHER_API_KEY}
      - MISTRAL_API_KEY=${MISTRAL_API_KEY}
      - GEMINI_API_TOKEN=${GEMINI_API_TOKEN}
      
      # Ollama integration for local LLM processing
      - OLLAMA_BASE_URL=http://ollama:11434
      - OLLAMA_HOST=ollama
      - OLLAMA_PORT=11434
      
      # Database connection
      - DATABASE_URL=postgresql://${POSTGRES_USER:-n8n}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-n8n}
      - POSTGRES_HOST=postgres
      - POSTGRES_PORT=5432
      
      # Crawl4AI Configuration
      - CRAWL4AI_HOST=0.0.0.0
      - CRAWL4AI_PORT=11235
      - CRAWL4AI_TIMEOUT_KEEP_ALIVE=300
      
      # Security settings for production
      - CRAWL4AI_SECURITY_ENABLED=false
      - CRAWL4AI_JWT_ENABLED=false
      - CRAWL4AI_RATE_LIMITING_ENABLED=true
      - CRAWL4AI_DEFAULT_LIMIT=1000/minute
      
      # Performance optimization
      - CRAWL4AI_MEMORY_THRESHOLD_PERCENT=95.0
      - CRAWL4AI_BATCH_PROCESS_TIMEOUT=300.0
      - CRAWL4AI_STREAM_INIT_TIMEOUT=30.0
      
      # Setup script configuration
      - ENABLE_MONITORING=false
    
    volumes:
      - shared_storage:/data/shared
      - /dev/shm:/dev/shm
      # Mount Crawl4AI configuration and scripts
      - ./crawl4ai/configs/crawl4ai-example-config.yml:/app/crawl4ai-example-config.yml:ro
      - ./crawl4ai/scripts/setup-crawl4ai.sh:/app/setup-crawl4ai.sh:ro
    
    command: 
      - /bin/bash
      - -c
      - |
        # Run setup script first
        /app/setup-crawl4ai.sh
        # Start the main Crawl4AI service
        exec crawl4ai
    
    healthcheck:
      test: ["CMD", "/app/health-check.sh"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s
    
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '1.0'
        reservations:
          memory: 1G
          cpus: '0.5'
    
    depends_on:
      postgres:
        condition: service_healthy
      ollama:
        condition: service_started
    
    logging:
      driver: "json-file"
      options:
        max-size: "100m"
        max-file: "3"
