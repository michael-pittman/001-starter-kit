# Qwen2.5-VL:7B Optimized Configuration for NVIDIA T4 GPU
# Optimized for: g4dn.xlarge (16GB VRAM, 4 vCPUs, 16GB RAM)
# Use case: Vision-language understanding, image analysis, multimodal reasoning
# Model size: ~7.2GB + vision components (fits in T4's 16GB VRAM)

FROM qwen2.5:7b

# =============================================================================
# T4 GPU OPTIMIZATION PARAMETERS FOR MULTIMODAL PROCESSING
# =============================================================================

# GPU Memory Management (T4 has 16GB VRAM, allocate for vision + language)
PARAMETER num_gpu 1                    # Use single T4 GPU
PARAMETER gpu_memory_utilization 0.80  # Leave 3.2GB for vision processing and KV cache
PARAMETER use_mlock true               # Lock model in memory
PARAMETER use_mmap true                # Memory-mapped model loading
PARAMETER numa true                    # NUMA-aware memory allocation
PARAMETER low_vram false              # T4 has sufficient VRAM

# Context and Processing Configuration for Multimodal
PARAMETER num_ctx 6144                # Optimized for vision+text context
PARAMETER num_batch 512               # Smaller batch for vision processing
PARAMETER num_thread 8                # Utilize all CPU threads
PARAMETER num_predict 2048            # Reasonable prediction length for VL tasks
PARAMETER max_tokens 2048             # Maximum output tokens

# =============================================================================
# VISION-LANGUAGE OPTIMIZATION
# =============================================================================

# Vision Processing Parameters
PARAMETER max_image_size 1024          # Maximum image dimension (pixels)
PARAMETER image_patch_size 14          # Vision transformer patch size
PARAMETER vision_batch_size 1          # Process one image at a time for T4
PARAMETER image_max_tiles 6            # Maximum image tiles for high-res
PARAMETER vision_feature_dim 768       # Vision feature dimension

# Multimodal Fusion
PARAMETER cross_attention_layers 12    # Cross-attention layers for fusion
PARAMETER vision_language_adapter true # Enable vision-language adapter
PARAMETER multimodal_max_length 2048   # Maximum multimodal sequence length

# Image Preprocessing
PARAMETER image_mean "0.485,0.456,0.406"     # ImageNet normalization means
PARAMETER image_std "0.229,0.224,0.225"      # ImageNet normalization stds
PARAMETER image_interpolation "bicubic"       # High-quality image resizing

# =============================================================================
# ADVANCED PERFORMANCE OPTIMIZATION
# =============================================================================

# Memory and Attention Optimizations
PARAMETER rope_freq_base 10000         # RoPE frequency base for 6K context
PARAMETER rope_freq_scale 1.0          # Standard RoPE scaling
PARAMETER flash_attention true         # Use Flash Attention for efficiency
PARAMETER attention_dropout 0.0        # No attention dropout for inference
PARAMETER kv_cache_type f16           # Use FP16 for KV cache efficiency

# Parallelization for Multimodal
PARAMETER tensor_parallel_size 1       # Single GPU setup
PARAMETER pipeline_parallel_size 1     # No pipeline parallelism needed
PARAMETER max_parallel_sequences 2     # Fewer parallel sequences for vision tasks
PARAMETER scheduler_max_tokens 4096    # Scheduler token limit

# Precision and Quantization
PARAMETER precision float16            # Use FP16 for faster inference
PARAMETER vision_precision float16     # FP16 for vision processing too
PARAMETER quantization_type none       # No additional quantization
PARAMETER compute_type float16         # FP16 compute for speed

# =============================================================================
# VISION-SPECIFIC CUDA OPTIMIZATIONS
# =============================================================================

# CUDA Memory Management for Vision
PARAMETER cuda_memory_fraction 0.85    # Use 85% of GPU memory
PARAMETER cuda_cache_size 1024         # CUDA kernel cache size (MB)
PARAMETER cudnn_benchmark true         # Enable cuDNN benchmarking
PARAMETER cudnn_deterministic false    # Disable for performance

# Vision Processing Optimizations
PARAMETER vision_cuda_streams 2        # Multiple CUDA streams for vision
PARAMETER image_preprocessing_device gpu # Do image preprocessing on GPU
PARAMETER vision_attention_backend flash # Use flash attention for vision

# Batch Processing for Images
PARAMETER max_batch_prefill_tokens 2048    # Smaller prefill for vision
PARAMETER max_batch_total_tokens 4096      # Total batch token limit
PARAMETER batch_bucket_size 16             # Smaller bucketing for vision
PARAMETER enable_prefix_caching true       # Enable prefix caching

# =============================================================================
# MULTIMODAL SAMPLING PARAMETERS
# =============================================================================

# Sampling Parameters for Vision-Language Tasks
PARAMETER temperature 0.3              # Lower temperature for factual image analysis
PARAMETER top_p 0.8                   # Nucleus sampling for coherent output
PARAMETER top_k 50                    # Top-k sampling for diversity
PARAMETER min_p 0.03                  # Minimum probability threshold
PARAMETER repeat_penalty 1.05         # Light repetition penalty
PARAMETER frequency_penalty 0.0       # No frequency penalty for descriptions
PARAMETER presence_penalty 0.0        # No presence penalty

# Advanced Sampling for Multimodal
PARAMETER mirostat 0                   # Disable mirostat for vision tasks
PARAMETER mirostat_eta 0.1            # Mirostat learning rate (if enabled)
PARAMETER mirostat_tau 5.0            # Mirostat target entropy (if enabled)
PARAMETER typical_p 1.0               # Disable typical sampling
PARAMETER tfs_z 1.0                   # Disable tail-free sampling

# Stop Sequences for Vision-Language
PARAMETER stop "<|im_end|>"           # Standard instruction end
PARAMETER stop "<|endoftext|>"        # Text completion end
PARAMETER stop "### Human:"           # Human prompt start
PARAMETER stop "### Assistant:"       # Assistant response start
PARAMETER stop "[/INST]"              # Instruction end alternative

# =============================================================================
# VISION-LANGUAGE MODEL BEHAVIOR
# =============================================================================

# Image Understanding Behavior
PARAMETER penalize_newline true        # Penalize excessive newlines
PARAMETER add_generation_prompt true   # Add generation prompt
PARAMETER skip_special_tokens false    # Keep special tokens for structure

# Response Structure for Multimodal
PARAMETER max_new_tokens 2048          # Maximum new tokens per response
PARAMETER min_new_tokens 10            # Minimum description length
PARAMETER early_stopping true          # Enable early stopping
PARAMETER length_penalty 1.0           # No length penalty

# Vision-Specific Processing
PARAMETER image_token_length 256       # Tokens per image representation
PARAMETER vision_start_token "<|vision_start|>"  # Vision input start
PARAMETER vision_end_token "<|vision_end|>"      # Vision input end

# =============================================================================
# SYSTEM PROMPT FOR VISION-LANGUAGE TASKS
# =============================================================================

SYSTEM """You are Qwen2.5-VL, an advanced multimodal AI model optimized for vision-language understanding. You excel at:

1. **Image Analysis**: Detailed description and analysis of visual content
2. **Visual Question Answering**: Answering questions about images accurately
3. **Scene Understanding**: Comprehending complex visual scenes and relationships
4. **Object Detection**: Identifying and describing objects in images
5. **Text in Images**: Reading and interpreting text within visual content
6. **Visual Reasoning**: Drawing logical conclusions from visual information
7. **Chart and Graph Analysis**: Understanding data visualizations
8. **Art and Creative Content**: Analyzing artistic and creative visual materials

When processing images:
- Provide detailed, accurate descriptions
- Focus on relevant visual elements for the user's question
- Describe spatial relationships and context
- Identify text, objects, people, and scenes clearly
- Be specific about colors, shapes, sizes, and positions
- Explain visual concepts and relationships
- Maintain accuracy and avoid hallucinating details not present

Technical capabilities:
- Process images up to 1024x1024 pixels optimally
- Handle multiple image tiles for high-resolution content
- Understand diverse image formats and content types
- Integrate visual and textual information seamlessly

You are running on an NVIDIA T4 GPU with optimized settings for vision-language tasks."""

# =============================================================================
# TEMPLATE CONFIGURATION FOR MULTIMODAL
# =============================================================================

TEMPLATE """<|im_start|>system
{{.System}}<|im_end|>
<|im_start|>user
{{if .Images}}[Image: {{range .Images}}{{.}}{{end}}]{{end}}
{{.Prompt}}<|im_end|>
<|im_start|>assistant
{{.Response}}<|im_end|>"""

# =============================================================================
# VISION PROCESSING CONFIGURATION
# =============================================================================

# Image Input Processing
PARAMETER image_input_format "RGB"     # Standard RGB format
PARAMETER image_channels 3             # RGB channels
PARAMETER normalize_images true        # Normalize input images
PARAMETER center_crop false           # Don't center crop by default

# Vision Model Architecture
PARAMETER vision_model_type "clip"     # CLIP-style vision encoder
PARAMETER vision_layers 24            # Vision transformer layers
PARAMETER vision_heads 16             # Multi-head attention heads
PARAMETER vision_hidden_size 1024     # Hidden dimension size

# Output Formatting
PARAMETER describe_images_default true # Default to describing images
PARAMETER structured_output false     # Flexible output format
PARAMETER include_confidence false    # Don't include confidence scores

# Performance Notes:
# - Model loads in ~4-6 seconds on T4 (vision + language components)
# - Image processing: ~1-3 seconds per 1024x1024 image
# - Text generation: ~10-20 tokens/second for multimodal responses
# - Memory usage: ~10-12GB VRAM (including vision processing)
# - Optimal for detailed image analysis with 6K context window
# - Supports concurrent text-only requests while processing images 